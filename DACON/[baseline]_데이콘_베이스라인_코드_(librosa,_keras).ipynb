{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "[baseline]_데이콘 베이스라인 코드 (librosa, keras).ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonhyuk0922/kaggle_study/blob/main/%5Bbaseline%5D_%EB%8D%B0%EC%9D%B4%EC%BD%98_%EB%B2%A0%EC%9D%B4%EC%8A%A4%EB%9D%BC%EC%9D%B8_%EC%BD%94%EB%93%9C_(librosa%2C_keras).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfPN1iiiwvzw",
        "outputId": "4a50d74e-10c6-482d-f977-e42c885d8ddc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6FULaVgurKP"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "import librosa\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuoi2CAowtZr"
      },
      "source": [
        "#압축 해제 및 tmp 디렉토리 저장\n",
        "import zipfile\n",
        "\n",
        "open_zip ='/content/drive/MyDrive/HackerTon/데이콘_음성인식/open.zip'\n",
        "zip_ref = zipfile.ZipFile(open_zip,'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAbycEBYurKS"
      },
      "source": [
        "### 데이터 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBn_YJkWurKS"
      },
      "source": [
        "sample_submission = pd.read_csv(\"/tmp/sample_submission.csv\")\n",
        "\n",
        "africa_train_paths = glob(\"./tmp/train/africa/*.wav\")\n",
        "australia_train_paths = glob(\"./tmp/train/australia/*.wav\")\n",
        "canada_train_paths = glob(\"./tmp/train/canada/*.wav\")\n",
        "england_train_paths = glob(\"./tmp/train/england/*.wav\")\n",
        "hongkong_train_paths = glob(\"./tmp/train/hongkong/*.wav\")\n",
        "us_train_paths = glob(\"./tmp/train/us/*.wav\")\n",
        "\n",
        "path_list = [africa_train_paths, australia_train_paths, canada_train_paths,\n",
        "             england_train_paths, hongkong_train_paths, us_train_paths]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Thw3HJ6PurKT",
        "outputId": "286cde87-a1b7-4b64-ad75-6adbb08516a5"
      },
      "source": [
        "# glob로 test data의 path를 불러올때 순서대로 로드되지 않을 경우를 주의해야 합니다.\n",
        "# test_ 데이터 프레임을 만들어서 나중에 sample_submission과 id를 기준으로 merge시킬 준비를 합니다.\n",
        "\n",
        "def get_id(data):\n",
        "    return np.int(data.split(\"/\")[-1].split(\".\")[0])\n",
        "\n",
        "test_ = pd.DataFrame(index = range(0, 6100), columns = [\"path\", \"id\"])\n",
        "test_[\"path\"] = glob(\"/tmp/test/*.wav\")\n",
        "test_[\"id\"] = test_[\"path\"].apply(lambda x : get_id(x))\n",
        "\n",
        "test_.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/tmp/test/4823.wav</td>\n",
              "      <td>4823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/tmp/test/2847.wav</td>\n",
              "      <td>2847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/tmp/test/4490.wav</td>\n",
              "      <td>4490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/tmp/test/3246.wav</td>\n",
              "      <td>3246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/tmp/test/277.wav</td>\n",
              "      <td>277</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 path    id\n",
              "0  /tmp/test/4823.wav  4823\n",
              "1  /tmp/test/2847.wav  2847\n",
              "2  /tmp/test/4490.wav  4490\n",
              "3  /tmp/test/3246.wav  3246\n",
              "4   /tmp/test/277.wav   277"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afaYSH52urKU"
      },
      "source": [
        "### 데이터 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFM5I8ScurKU"
      },
      "source": [
        "baseline 코드에서는 librosa 라이브러리를 사용하여 wav파일을 전처리 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTxql-d9urKV"
      },
      "source": [
        "def load_data(paths):\n",
        "\n",
        "    result = []\n",
        "    for path in tqdm(paths):\n",
        "        # sr = 16000이 의미하는 것은 1초당 16000개의 데이터를 샘플링 한다는 것입니다.\n",
        "        data, sr = librosa.load(path, sr = 16000)\n",
        "        result.append(data)\n",
        "    result = np.array(result) \n",
        "    # 메모리가 부족할 때는 데이터 타입을 변경해 주세요 ex) np.array(data, dtype = np.float32)\n",
        "\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sr2CY8AXurKV",
        "outputId": "5039ddb3-6f50-4905-c11f-6435c2cbda13"
      },
      "source": [
        "# train 데이터를 로드하기 위해서는 많은 시간이 소모 됩니다.\n",
        "# 따라서 추출된 정보를 npy파일로 저장하여 필요 할 때마다 불러올 수 있게 준비합니다.\n",
        "\n",
        "os.mkdir(\"./npy_data\")\n",
        "\n",
        "africa_train_data = load_data(africa_train_paths)\n",
        "np.save(\"./npy_data/africa_npy\", africa_train_data)\n",
        "\n",
        "australia_train_data = load_data(australia_train_paths)\n",
        "np.save(\"./npy_data/australia_npy\", australia_train_data)\n",
        "\n",
        "canada_train_data = load_data(canada_train_paths)\n",
        "np.save(\"./npy_data/canada_npy\", canada_train_data)\n",
        "\n",
        "england_train_data = load_data(england_train_paths)\n",
        "np.save(\"./npy_data/england_npy\", england_train_data)\n",
        "\n",
        "hongkong_train_data = load_data(hongkong_train_paths)\n",
        "np.save(\"./npy_data/hongkong_npy\", hongkong_train_data)\n",
        "\n",
        "us_train_data = load_data(us_train_paths)\n",
        "np.save(\"./npy_data/us_npy\", us_train_data)\n",
        "\n",
        "test_data = load_data(test_[\"path\"])\n",
        "np.save(\"./npy_data/test_npy\", test_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2500/2500 [09:52<00:00,  4.22it/s]\n",
            "100%|██████████| 1000/1000 [03:53<00:00,  4.29it/s]\n",
            "100%|██████████| 1000/1000 [03:50<00:00,  4.33it/s]\n",
            "100%|██████████| 10000/10000 [38:05<00:00,  4.37it/s]\n",
            "100%|██████████| 1020/1020 [04:53<00:00,  3.48it/s]\n",
            "100%|██████████| 10000/10000 [38:40<00:00,  4.31it/s]\n",
            "100%|██████████| 6100/6100 [26:27<00:00,  3.84it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2SjrdFrurKV"
      },
      "source": [
        "# npy파일로 저장된 데이터를 불러옵니다.\n",
        "africa_train_data = np.load(\"./npy_data/africa_npy.npy\", allow_pickle = True)\n",
        "australia_train_data = np.load(\"./npy_data/australia_npy.npy\", allow_pickle = True)\n",
        "canada_train_data = np.load(\"./npy_data/canada_npy.npy\", allow_pickle = True)\n",
        "england_train_data = np.load(\"./npy_data/england_npy.npy\", allow_pickle = True)\n",
        "hongkong_train_data = np.load(\"./npy_data/hongkong_npy.npy\", allow_pickle = True)\n",
        "us_train_data = np.load(\"./npy_data/us_npy.npy\", allow_pickle = True)\n",
        "\n",
        "test_data = np.load(\"./npy_data/test_npy.npy\", allow_pickle = True)\n",
        "\n",
        "train_data_list = [africa_train_data, australia_train_data, canada_train_data, england_train_data, hongkong_train_data, us_train_data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFtkeou5urKW"
      },
      "source": [
        "# 이번 대회에서 음성은 각각 다른 길이를 갖고 있습니다.\n",
        "# baseline 코드에서는 음성 중 길이가 가장 작은 길이의 데이터를 기준으로 데이터를 잘라서 사용합니다.\n",
        "\n",
        "def get_mini(data):\n",
        "\n",
        "    mini = 9999999\n",
        "    for i in data:\n",
        "        if len(i) < mini:\n",
        "            mini = len(i)\n",
        "\n",
        "    return mini\n",
        "\n",
        "#음성들의 길이를 맞춰줍니다.\n",
        "\n",
        "def set_length(data, d_mini):\n",
        "\n",
        "    result = []\n",
        "    for i in data:\n",
        "        result.append(i[:d_mini])\n",
        "    result = np.array(result)\n",
        "\n",
        "    return result\n",
        "\n",
        "#feature를 생성합니다.\n",
        "\n",
        "def get_feature(data, sr = 16000, n_fft = 256, win_length = 200, hop_length = 160, n_mels = 64):\n",
        "    mel = []\n",
        "    for i in data:\n",
        "        # win_length 는 음성을 작은 조각으로 자를때 작은 조각의 크기입니다.\n",
        "        # hop_length 는 음성을 작은 조각으로 자를때 자르는 간격을 의미합니다.\n",
        "        # n_mels 는 적용할 mel filter의 개수입니다.\n",
        "        mel_ = librosa.feature.melspectrogram(i, sr = sr, n_fft = n_fft, win_length = win_length, hop_length = hop_length, n_mels = n_mels)\n",
        "        mel.append(mel_)\n",
        "    mel = np.array(mel)\n",
        "    mel = librosa.power_to_db(mel, ref = np.max)\n",
        "\n",
        "    mel_mean = mel.mean()\n",
        "    mel_std = mel.std()\n",
        "    mel = (mel - mel_mean) / mel_std\n",
        "\n",
        "    return mel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "5ZzYaMINurKW",
        "outputId": "42d04ed0-97a0-438b-8316-08cbf40a82dd"
      },
      "source": [
        "train_x = np.concatenate(train_data_list, axis= 0)\n",
        "test_x = np.array(test_data)\n",
        "\n",
        "# 음성의 길이 중 가장 작은 길이를 구합니다.\n",
        "\n",
        "train_mini = get_mini(train_x)\n",
        "test_mini = get_mini(test_x)\n",
        "\n",
        "mini = np.min([train_mini, test_mini])\n",
        "\n",
        "# data의 길이를 가장 작은 길이에 맞춰 잘라줍니다.\n",
        "\n",
        "train_x = set_length(train_x, mini)\n",
        "test_x = set_length(test_x, mini)\n",
        "\n",
        "train_x = train_x.reshape(-1, train_x.shape[1], train_x.shape[2], 1)\n",
        "test_x = test_x.reshape(-1, test_x.shape[1], test_x.shape[2], 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-d5e54f7fae6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtest_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mtest_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cobQpbqOUCp_"
      },
      "source": [
        "# librosa를 이용해 feature를 추출합니다.\n",
        "\n",
        "train_x = get_feature(data = train_x)\n",
        "test_x = get_feature(data = test_x)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBXzlfW1urKX"
      },
      "source": [
        "# train_data의 label을 생성해 줍니다.\n",
        "\n",
        "train_y = np.concatenate((np.zeros(len(africa_train_data), dtype = np.int),\n",
        "                        np.ones(len(australia_train_data), dtype = np.int),\n",
        "                         np.ones(len(canada_train_data), dtype = np.int) * 2,\n",
        "                         np.ones(len(england_train_data), dtype = np.int) * 3,\n",
        "                         np.ones(len(hongkong_train_data), dtype = np.int) * 4,\n",
        "                         np.ones(len(us_train_data), dtype = np.int) * 5), axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ApXU7R5urKX",
        "outputId": "ce502ec8-6ebd-4e6e-8ac1-c6d79ea8a68a"
      },
      "source": [
        "train_x.shape, train_y.shape, test_x.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((25520, 80064), (25520,), (6100, 80064))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0B4nazcHurKY"
      },
      "source": [
        "### 분석 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "292ATTYrurKY"
      },
      "source": [
        " 분석 모델은 월간데이콘_6 음성 중첩 데이터 분류 AI 경진대회 3위를 달성하신 Jamm님의 코드를 바탕으로 만들어졌습니다.  \n",
        " https://www.dacon.io/competitions/official/235616/codeshare/1571?page=1&dtype=recent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heB41pdgurKY"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (Input, Convolution2D, BatchNormalization, Flatten,\n",
        "                                     Dropout, Dense, AveragePooling2D, Add)\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hflmr8UnurKY"
      },
      "source": [
        "def block(input_, units = 32, dropout_rate = 0.5):\n",
        "    \n",
        "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(input_)\n",
        "    x = BatchNormalization()(x)\n",
        "    x_res = x\n",
        "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Add()([x, x_res])\n",
        "    x = AveragePooling2D()(x)\n",
        "    x = Dropout(rate=dropout_rate)(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "def second_block(input_, units = 64, dropout_rate = 0.5):\n",
        "    \n",
        "    x = Convolution2D(units, 1, padding =\"same\", activation = \"relu\")(input_)\n",
        "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(x)\n",
        "    x = Convolution2D(units * 4, 1, padding =\"same\", activation = \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x_res = x\n",
        "    x = Convolution2D(units, 1, padding =\"same\", activation = \"relu\")(x)\n",
        "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(x)\n",
        "    x = Convolution2D(units * 4, 1, padding =\"same\", activation = \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Convolution2D(units, 1, padding = \"same\", activation = \"relu\")(x)\n",
        "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(x)\n",
        "    x = Convolution2D(units * 4, 1, padding = \"same\", activation = \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Add()([x, x_res])\n",
        "    x = AveragePooling2D()(x)\n",
        "    x = Dropout(rate=dropout_rate)(x)\n",
        "    \n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6fjIPoeurKZ"
      },
      "source": [
        "def build_fn():\n",
        "    dropout_rate = 0.3\n",
        "    \n",
        "    in_ = Input(shape = (train_x.shape[1:]))\n",
        "    \n",
        "    block_01 = block(in_, units = 32, dropout_rate = dropout_rate)\n",
        "    block_02 = block(block_01, units = 64, dropout_rate = dropout_rate)\n",
        "    block_03 = block(block_02, units = 128, dropout_rate = dropout_rate)\n",
        "\n",
        "    block_04 = second_block(block_03, units = 64, dropout_rate = dropout_rate)\n",
        "    block_05 = second_block(block_04, units = 128, dropout_rate = dropout_rate)\n",
        "\n",
        "    x = Flatten()(block_05)\n",
        "\n",
        "    x = Dense(units = 128, activation = \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x_res = x\n",
        "    x = Dropout(rate = dropout_rate)(x)\n",
        "\n",
        "    x = Dense(units = 128, activation = \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Add()([x_res, x])\n",
        "    x = Dropout(rate = dropout_rate)(x)\n",
        "\n",
        "    model_out = Dense(units = 6, activation = 'softmax')(x)\n",
        "    model = Model(in_, model_out)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3k9WMsburKZ"
      },
      "source": [
        "### 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "pS2bMXalurKZ"
      },
      "source": [
        "split = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\n",
        "\n",
        "pred = []\n",
        "pred_ = []\n",
        "\n",
        "for train_idx, val_idx in split.split(train_x, train_y):\n",
        "    x_train, y_train = train_x[train_idx], train_y[train_idx]\n",
        "    x_val, y_val = train_x[val_idx], train_y[val_idx]\n",
        "\n",
        "    model = build_fn()\n",
        "    model.compile(optimizer = keras.optimizers.Adam(0.001),\n",
        "                 loss = keras.losses.SparseCategoricalCrossentropy(),\n",
        "                 metrics = ['acc'])\n",
        "\n",
        "    history = model.fit(x = x_train, y = y_train, validation_data = (x_val, y_val), epochs = 16)\n",
        "    print(\"*******************************************************************\")\n",
        "    pred.append(model.predict(test_x))\n",
        "    pred_.append(np.argmax(model.predict(test_x), axis = 1))\n",
        "    print(\"*******************************************************************\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-GuDkRturKa"
      },
      "source": [
        "### 예측하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vT3uWelsurKa"
      },
      "source": [
        "def cov_type(data):\n",
        "    return np.int(data)\n",
        "\n",
        "# 처음에 살펴본 것처럼 glob로 test data의 path는 sample_submission의 id와 같이 1,2,3,4,5.....으로 정렬 되어있지 않습니다.\n",
        "# 만들어둔 test_ 데이터프레임을 이용하여 sample_submission과 predict값의 id를 맞춰줍니다.\n",
        "\n",
        "result = pd.concat([test_, pd.DataFrame(np.mean(pred, axis = 0))], axis = 1).iloc[:, 1:]\n",
        "result[\"id\"] = result[\"id\"].apply(lambda x : cov_type(x))\n",
        "\n",
        "result = pd.merge(sample_submission[\"id\"], result)\n",
        "result.columns = sample_submission.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJqeD1K6urKa"
      },
      "source": [
        "result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckdCRKW7urKa"
      },
      "source": [
        "result.to_csv(\"DACON.csv\", index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFjtl1mXurKa"
      },
      "source": [
        "baseline은 참가자의 제출을 최우선 목표로 하고 있습니다.  \n",
        "창의적인 전처리 방법을 적용하고 훌륭한 분석 모델을 개발해 주세요  \n",
        "  \n",
        "감사합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOteHJ3NurKb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
